version: '3.8'

services:
  tailscale:
    image: tailscale/tailscale:latest
    hostname: tailscale
    environment:
      - TS_AUTHKEY=tskey-auth-k5gUCHV3DN11CNTRL-eQAy4NhmqggbnmQvRxyDhgyQYSng8SBe
      - TS_EXTRA_ARGS=--advertise-tags=tag:container
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_EXTRA_ARGS=--advertise-routes=10.0.0.0/24 --accept-routes
    volumes:
      - ./tailscale/state:/var/lib/tailscale
    devices:
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    restart: unless-stopped
  postgres: # For N8N
    image: postgres:16
    restart: always
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
      - POSTGRES_NON_ROOT_USER
      - POSTGRES_NON_ROOT_PASSWORD
    volumes:
      - db_storage:/var/lib/postgresql/data
      - ./init-data.sh:/docker-entrypoint-initdb.d/init-data.sh
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}']
      interval: 5s
      timeout: 5s
      retries: 10
    networks:
      - main-network

  n8n:
    image: docker.n8n.io/n8nio/n8n
    restart: always
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
      - DB_POSTGRESDB_USER=${POSTGRES_NON_ROOT_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_NON_ROOT_PASSWORD}
      - N8N_SECURE_COOKIE=False
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - 5678:5678
    links:
      - postgres
    volumes:
      - n8n_storage:/home/node/.n8n
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - main-network
  kafka:
    image: apache/kafka:4.0.0
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      # Unique ID for this broker
      KAFKA_NODE_ID: 1

      # Cluster ID (generate once: `uuidgen`)
      KAFKA_CLUSTER_ID: "q1Sh-8BCRyKf-9pgaJ0Z2w"

      # Listeners - Separate listeners for internal and external access
      KAFKA_LISTENERS: PLAINTEXT://:29092,PLAINTEXT_HOST://:9092,CONTROLLER://:9093
      
      # Advertised listeners - Internal for container communication, External for localhost
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092

      # Storage directories (logs = Kafka data, not app logs)
      KAFKA_LOG_DIRS: /var/lib/kafka/data

      # KRaft mode settings - Fixed controller quorum voters
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # Inter-broker listener - Use PLAINTEXT for container communication
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # Security settings for controller
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT

      # Auto-create topics settings
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - 8080:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local-kafka
      # Use internal listener for container-to-container communication
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - kafka-network
      - main-network
  
  akhq:
    image: tchiotludo/akhq:latest
    container_name: akhq
    restart: unless-stopped
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            homelab-kafka:
              properties:
                # Use existing Kafka broker from the main stack
                bootstrap.servers: "kafka:29092"
              # Uncomment if you later add Schema Registry
              # schema-registry:
              #   url: "http://schema-registry:8085"
    # No host ports; Caddy will reverse-proxy
    networks:
      - main-network
      - kafka-network
  
  gatus:
    image: twinproduction/gatus:latest
    ports:
      - 8082:8080
    volumes:
      - ./gatus/config:/config
    networks:
      - main-network

  dozzle:
    container_name: dozzle
    image: amir20/dozzle:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - 8083:8080
    networks:
      - main-network

  cadvisor:
    image: ghcr.io/google/cadvisor:v0.53.0
    container_name: cadvisor
    restart: unless-stopped
    privileged: true
    ports: # don't need ports because caddy proxies to the container ports for us
      - "8089:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - main-network

  pihole:
    container_name: pihole
    image: pihole/pihole:latest
    ports:
      - "53:53/tcp"
      - "53:53/udp"
      # - "80:80/tcp" commented out because caddy uses these
      # - "443:443/tcp"
    environment:
      - TZ=${TZ}
      - FTLCONF_webserver_api_password=${PIHOLE_PASSWORD}
      - FTLCONF_dns_listeningMode=all
      #- FTLCONF_dns_domain=homelab
      - FTLCONF_misc_etc_dnsmasq_d=true
    volumes:
      - ~/docker-volumes/pihole-config/etc-pihole:/etc/pihole
      - ./pihole/etc-dnsmasq.d:/etc/dnsmasq.d
    cap_add:
      - NET_ADMIN
      - SYS_TIME
      - SYS_NICE
    restart: unless-stopped
    networks:
      - main-network


  prometheus:
    image: prom/prometheus:v2.47.2
    container_name: prometheus
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--web.enable-remote-write-receiver'
    networks:
      - main-network
    restart: unless-stopped

  loki:
    image: grafana/loki:2.9.2
    container_name: loki
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "3100:3100"
    volumes:
      - ./loki/loki.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - main-network
    restart: unless-stopped


  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    ports:
      - "12345:12345"  # Alloy UI
    user: "0"
    volumes:
      - ./alloy/config.alloy:/etc/alloy/config.alloy
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
    environment:
      - HOSTNAME
    networks:
      - main-network
    restart: unless-stopped
    depends_on:
      - prometheus
      - loki

  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "3000:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - main-network
    restart: unless-stopped
    depends_on:
      - prometheus
      - loki

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    hostname: metabase
    # Do not publish host ports; Caddy will proxy internally
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabaseappdb
      MB_DB_PORT: 5432
      MB_DB_USER: metabase
      MB_DB_PASS: ${METABASE_DB_PASSWORD}
      MB_DB_HOST: metabase-db
    depends_on:
      metabase-db:
        condition: service_healthy
    healthcheck:
      test: curl --fail -I http://localhost:3000/api/health || exit 1
      interval: 15s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - main-network

  metabase-db:
    image: postgres:16-alpine
    container_name: metabase_db
    hostname: metabase-db
    environment:
      POSTGRES_USER: metabase
      POSTGRES_DB: metabaseappdb
      POSTGRES_PASSWORD: ${METABASE_DB_PASSWORD}
    volumes:
      - metabase-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - main-network
  umami:
    image: ghcr.io/umami-software/umami:postgresql-latest
    # Do not publish host port 3000 to avoid conflict with Grafana
    environment:
      DATABASE_URL: postgresql://umami:umami@umami-db:5432/umami
      DATABASE_TYPE: postgresql
      APP_SECRET: ${UMAMI_APP_SECRET}
    depends_on:
      umami-db:
        condition: service_healthy
    init: true
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:3000/api/heartbeat"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - main-network

  umami-db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: umami
      POSTGRES_USER: umami
      POSTGRES_PASSWORD: umami
    volumes:
      - umami-db-data:/var/lib/postgresql/data
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - main-network
  homepage:
    image: ghcr.io/gethomepage/homepage:latest
    container_name: homepage
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "3001:3000" # Grafana uses 3000 on host
    volumes:
      - ./homepage/config:/app/config # Ensure this local dir exists
      - ~/docker-volumes/homepage-logs:/app/config/logs # don't want logs to be committed into the repo
      - /var/run/docker.sock:/var/run/docker.sock # Optional: for Docker integrations
    environment:
      HOMEPAGE_ALLOWED_HOSTS: localhost:3001,homepage.homelab
    restart: unless-stopped
    networks:
      - main-network
  cronmaster:
    image: ghcr.io/fccview/cronmaster:latest
    container_name: cronmaster
    user: "root"
    ports:
      - "40123:3000"
    environment:
      NODE_ENV: production
      DOCKER: "true"
      NEXT_PUBLIC_CLOCK_UPDATE_INTERVAL: 30000
      AUTH_PASSWORD: very_strong_password
      HOST_CRONTAB_USER: root
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ~/docker-volumes/cronmaster/scripts:/app/scripts
      - ~/docker-volumes/cronmaster/data:/app/data
      - ~/docker-volumes/cronmaster/snippets:/app/snippets
    pid: "host"
    privileged: true
    restart: always
    init: true
    networks:
      - main-network
  jotty:
    image: ghcr.io/fccview/jotty:latest
    container_name: jotty
    user: "1000:1000"
    ports:
      - "1122:3000"
    volumes:
      - ~/docker-volumes/jotty/data:/app/data:rw
      - ./jotty/config:/app/config:rw
      - ~/docker-volumes/jotty/cache:/app/.next/cache:rw
    restart: unless-stopped
    environment:
      NODE_ENV: production
    networks:
      - main-network

  uptime-kuma:
    image: louislam/uptime-kuma:2.0.2
    container_name: uptime-kuma
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "3002:3001" # 3001 is used by homepage on host
    volumes:
      - uptime_kuma_data:/app/data
    restart: unless-stopped
    networks:
      - main-network # so that uptime kuma can reach out to other services

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - kafka-network

  redisinsight:
    image: redis/redisinsight:latest
    container_name: redisinsight
    restart: unless-stopped
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "5540:5540"
    volumes:
      - redisinsight_data:/data
    environment:
      - RI_APP_PORT=5540
      - RI_APP_HOST=0.0.0.0
      - RI_ENCRYPTION_KEY=${RI_ENCRYPTION_KEY}
      - RI_LOG_LEVEL=info
      - RI_FILES_LOGGER=true
      - RI_STDOUT_LOGGER=true
      - RI_ACCEPT_TERMS_AND_CONDITIONS=true
      - RI_DATABASE_MANAGEMENT=true
      # Preconfigure a Redis connection (ID defaults to 0 when omitted)
      - RI_REDIS_HOST=redis
      - RI_REDIS_PORT=6379
      - RI_REDIS_ALIAS=Primary Redis
      - RI_REDIS_USERNAME=default
      - RI_REDIS_PASSWORD=${REDIS_PASSWORD}
      - RI_REDIS_TLS=false
      - RI_REDIS_DB=0
    depends_on:
      - redis
    networks:
      - main-network

  paperless-ngx-broker:
    image: docker.io/library/redis:8
    container_name: paperless-ngx-broker
    restart: unless-stopped
    volumes:
      - paperless-ngx-redisdata:/data
    networks:
      - main-network

  paperless-ngx-db:
    image: docker.io/library/postgres:18
    container_name: paperless-ngx-db
    restart: unless-stopped
    volumes:
      - paperless-ngx-pgdata:/var/lib/postgresql
    environment:
      POSTGRES_DB: paperless
      POSTGRES_USER: paperless
      POSTGRES_PASSWORD: paperless
    networks:
      - main-network

  paperless-ngx-webserver:
    image: ghcr.io/paperless-ngx/paperless-ngx:latest
    container_name: paperless-ngx-webserver
    restart: unless-stopped
    depends_on:
      - paperless-ngx-db
      - paperless-ngx-broker
    # ports:
    #   - "8000:8000"  # Exposed via Caddy; internal only
    volumes:
      - paperless-ngx-data:/usr/src/paperless/data
      - paperless-ngx-media:/usr/src/paperless/media
      - ./paperless-ngx/export:/usr/src/paperless/export
      - ./paperless-ngx/consume:/usr/src/paperless/consume
    env_file: ./paperless-ngx/docker-compose.env
    environment:
      PAPERLESS_REDIS: redis://paperless-ngx-broker:6379
      PAPERLESS_DBHOST: paperless-ngx-db
    networks:
      - main-network

  beszel:
    image: henrygd/beszel:latest
    container_name: beszel
    restart: unless-stopped
    ports:
      - "8090:8090"
    volumes:
      - ~/docker-volumes/beszel/beszel_data:/beszel_data
      - ~/docker-volumes/beszel/beszel_socket:/beszel_socket
    networks:
      - main-network
  # Runs on each system you want to monitor and communicates system metrics to the hub
  beszel-agent:
    image: henrygd/beszel-agent:latest
    container_name: beszel-agent
    restart: unless-stopped
    network_mode: host
    env_file:
      - ./beszel/.env
    volumes:
      - ~/docker-volumes/beszel/beszel_agent_data:/var/lib/beszel-agent
      - ~/docker-volumes/beszel/beszel_socket:/beszel_socket
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      LISTEN: /beszel_socket/beszel.sock
      HUB_URL: http://localhost:8090
      TOKEN: ${BESZEL_TOKEN}
      KEY: ${BESZEL_KEY}

  karakeep-web:
    image: ghcr.io/karakeep-app/karakeep:${KARAKEEP_VERSION:-release}
    container_name: karakeep-web
    restart: unless-stopped
    env_file:
      - ./karakeep/.env
    environment:
      MEILI_ADDR: http://karakeep-meilisearch:7700
      BROWSER_WEB_URL: http://karakeep-chrome:9222
      DATA_DIR: /data
    volumes:
      - karakeep-data:/data
    networks:
      - main-network

  karakeep-chrome:
    image: gcr.io/zenika-hub/alpine-chrome:124
    container_name: karakeep-chrome
    restart: unless-stopped
    command:
      - --no-sandbox
      - --disable-gpu
      - --disable-dev-shm-usage
      - --remote-debugging-address=0.0.0.0
      - --remote-debugging-port=9222
      - --hide-scrollbars
    networks:
      - main-network

  karakeep-meilisearch:
    image: getmeili/meilisearch:v1.13.3
    container_name: karakeep-meilisearch
    restart: unless-stopped
    env_file:
      - ./karakeep/.env
    environment:
      MEILI_NO_ANALYTICS: "true"
    volumes:
      - karakeep-meilisearch:/meili_data
    networks:
      - main-network

  netdata:
    image: netdata/netdata:latest
    container_name: netdata
    pid: host
    restart: unless-stopped
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - netdataconfig:/etc/netdata
      - netdatalib:/var/lib/netdata
      - netdatacache:/var/cache/netdata
      - /:/host/root:ro
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /run/dbus:/run/dbus:ro
    networks:
      - main-network

  komodo-mongo:
    image: mongo:latest
    container_name: komodo-mongo
    labels:
      komodo.skip: "true"
    command: --quiet --wiredTigerCacheSizeGB 0.25
    restart: unless-stopped
    # ports:
    #   - 27017:27017
    volumes:
      - komodo-mongo-data:/data/db
      - komodo-mongo-config:/data/configdb
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${KOMODO_DB_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${KOMODO_DB_PASSWORD}
    networks:
      - main-network

  komodo-core:
    image: ghcr.io/moghtech/komodo-core:${COMPOSE_KOMODO_IMAGE_TAG:-latest}
    container_name: komodo-core
    labels:
      komodo.skip: "true"
    restart: unless-stopped
    depends_on:
      - komodo-mongo
    ports:
      - 9120:9120
    env_file: ./komodo/compose.env
    environment:
      KOMODO_DATABASE_ADDRESS: komodo-mongo:27017
      KOMODO_DATABASE_USERNAME: ${KOMODO_DB_USERNAME}
      KOMODO_DATABASE_PASSWORD: ${KOMODO_DB_PASSWORD}
    volumes:
      ## Store dated backups of the database - https://komo.do/docs/setup/backup
      - ${COMPOSE_KOMODO_BACKUPS_PATH:-./komodo/backups}:/backups
      ## Store sync files on server
      # - /path/to/syncs:/syncs
      ## Optionally mount a custom core.config.toml
      # - /path/to/core.config.toml:/config/config.toml
    ## Allows for systemd Periphery connection at 
    ## "https://host.docker.internal:8120"
    # extra_hosts:
    #   - host.docker.internal:host-gateway

    networks:
      - main-network

  komodo-periphery:
    image: ghcr.io/moghtech/komodo-periphery:${COMPOSE_KOMODO_IMAGE_TAG:-latest}
    container_name: komodo-periphery
    labels:
      komodo.skip: "true"
    restart: unless-stopped
    env_file: ./komodo/compose.env
    volumes:
      ## Mount external docker socket
      - /var/run/docker.sock:/var/run/docker.sock
      ## Allow Periphery to see processes outside of container
      - /proc:/proc
      ## Specify the Periphery agent root directory.
      ## Must be the same inside and outside the container,
      ## or docker will get confused. See https://github.com/moghtech/komodo/discussions/180.
      ## Default: /etc/komodo.
      - ${PERIPHERY_ROOT_DIRECTORY:-/etc/komodo}:${PERIPHERY_ROOT_DIRECTORY:-/etc/komodo}
    networks:
      - main-network

  excalidraw:
    image: excalidraw/excalidraw:latest
    container_name: excalidraw
    restart: unless-stopped
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "5000:80"
    networks:
      - main-network

  changedetection:
    image: dgtlmoon/changedetection.io:latest
    container_name: changedetection
    # ports:
    #   - "5000:5000"  # Exposed via Caddy; internal only
    volumes:
      - datastore-volume:/datastore
    restart: unless-stopped
    networks:
      - main-network

  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: ollama
    # ports:
    #   - "7869:11434"  # Exposed via Caddy; internal only
    volumes:
      - ~/docker-volumes/ollama/ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    tty: true
    pull_policy: always
    restart: unless-stopped
    networks:
      - main-network

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    # ports:
    #   - "8080:8080"  # Exposed via Caddy; internal only
    volumes:
      - ~/docker-volumes/ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URLS=http://ollama:11434
      - ENV=dev
      - WEBUI_AUTH=False
      - WEBUI_NAME=valiantlynx AI
      - WEBUI_URL=http://ollama-webui.homelab
      - WEBUI_SECRET_KEY=t0p-s3cr3t
    restart: unless-stopped
    networks:
      - main-network

  jellyfin:
    image: jellyfin/jellyfin
    container_name: jellyfin
    # user: "1000:1000"
    volumes:
      - ~/docker-volumes/jellyfin/config:/config
      - ~/docker-volumes/jellyfin/cache:/cache
      # Media libraries: update these with your actual host paths.
      # - type: bind
      #   source: /mnt/media
      #   target: /media
      # - type: bind
      #   source: /mnt/media2
      #   target: /media2
      #   read_only: true
      # Optional - extra fonts to be used during transcoding with subtitle burn-in
      # - type: bind
      #   source: /usr/local/share/fonts
      #   target: /usr/local/share/fonts/custom
      #   read_only: true
    restart: 'unless-stopped'
    # Optional - alternative address used for autodiscovery
    environment:
      - JELLYFIN_PublishedServerUrl=http://example.com
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "8096:8096"
    networks:
      - main-network

  pinchflat:
    image: ghcr.io/kieraneglin/pinchflat:latest
    platform: linux/x86_64 # to fix error: no matching manifest for linux/arm64/v8 in the manifest list entries
    container_name: pinchflat
    restart: unless-stopped
    environment:
      - TZ=${TZ}
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "8945:8945"
    volumes:
      - ~/docker-volumes/pinchflat/config:/config
      - ~/docker-volumes/pinchflat/downloads:/downloads
    networks:
      - main-network

  glance:
    container_name: glance
    image: glanceapp/glance
    restart: unless-stopped
    volumes:
      - ./glance/config:/app/config
      - ./glance/assets:/app/assets
      - /etc/localtime:/etc/localtime:ro
      # Optionally, also mount docker socket if you want to use the docker containers widget
      # - /var/run/docker.sock:/var/run/docker.sock:ro
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - 8084:8080
    environment:
      - MY_SECRET_TOKEN=123456
    networks:
      - main-network


  flame:
    image: pawelmalak/flame
    container_name: flame
    restart: unless-stopped
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "5005:5005"
    environment:
      - PASSWORD=${FLAME_PASSWORD}
    volumes:
      - ~/docker-volumes/flame:/app/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - main-network



  test-db:
    image: postgres:16
    container_name: postgres_db
    restart: always
    environment:
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpassword
      POSTGRES_DB: test_database
    ports:
      - "5432:5432"
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
      - ./test-db-init.sql:/docker-entrypoint-initdb.d/10-test-table.sql:ro

# https://github.com/loganphillips792/live-auction
  live-auction:
    image: dockedupstream/live-auction:main
    # ports: # don't need ports because caddy proxies to the container ports for us
    #   - "8000:8000"
    env_file:
      - live-auction/.env
    environment:
      - OTEL_SERVICE_NAME=live-auction-backend
      - OTEL_EXPORTER_OTLP_ENDPOINT=tempo:4317 # Removed http://
    volumes:
      - ./live-auction/db:/app/db
    networks:
      - main-network
    # depends_on: # Ensure backend starts after tempo is available
    #   - tempo


  caddy:
    image: caddy:latest
    container_name: caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - ./caddy:/etc/caddy
      - ./caddy/site:/srv
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - default
      - main-network
      - kafka-network
  
  backrest:
    image: garethgeorge/backrest:latest
    container_name: backrest
    hostname: backrest
    restart: unless-stopped
    environment:
      - BACKREST_DATA=/data
      - BACKREST_CONFIG=/config/config.json
      - XDG_CACHE_HOME=/cache
      - TMPDIR=/tmp
      - TZ=${TZ}
    volumes:
      - ./backrest/data:/data
      - ./backrest/config:/config
      - ./backrest/cache:/cache
      - ./backrest/tmp:/tmp
      - ./backrest/rclone:/root/.config/rclone
      - ${BACKREST_USERDATA:-/Users/logan/docker-volumes}:/userdata
      - ${BACKREST_REPOS:-/Users/logan/repos}:/repos
    networks:
      - main-network

  cta-map:
    image: dockedupstream/cta-map:latest
    container_name: cta-map
    restart: unless-stopped
    env_file:
      - ./cta-map/compose.env
    environment:
      - PORT=8080
    volumes:
      - ~/docker-volumes/cta-map:/app/data:ro
    networks:
      - main-network


volumes:
  kafka-data:
  db_storage:
  n8n_storage:
  prometheus_data:
  loki_data:
  grafana_data:
  pihole_etc:
  uptime_kuma_data:
  metabase-db-data:
  umami-db-data:
  postgres_test_data:
  caddy_data:
  caddy_config:
  redis_data:
  redisinsight_data:
  datastore-volume:
  komodo-mongo-data:
  komodo-mongo-config:
  netdataconfig:
  netdatalib:
  netdatacache:
  karakeep-meilisearch:
  karakeep-data:
  paperless-ngx-data:
  paperless-ngx-media:
  paperless-ngx-pgdata:
  paperless-ngx-redisdata:

networks:
  kafka-network:
    driver: bridge
  main-network:
    driver: bridge
